# Backend Environment Configuration
# Copy this file to .env for local development
# Copy this file to .env.production for production

# Server Configuration
HOST=0.0.0.0
PORT=8000
DEBUG=true

# CORS Configuration
CORS_ORIGINS=http://localhost:3000,http://localhost:8080,http://localhost:9000,http://localhost:9001,http://127.0.0.1:3000,http://127.0.0.1:8080,http://127.0.0.1:9000,http://127.0.0.1:9001

# Model Configuration
WHISPER_MODEL_NAME=openai/whisper-large-v3
WHISPER_MAX_LENGTH=448
WHISPER_NUM_BEAMS=1
WHISPER_DO_SAMPLE=false
WHISPER_EARLY_STOPPING=true

# Audio Configuration
AUDIO_SAMPLE_RATE=16000
AUDIO_BUFFER_DURATION_SECONDS=3.0
AUDIO_CHUNK_OVERLAP=0.1
AUDIO_SILENCE_THRESHOLD=0.01
AUDIO_MIN_LENGTH=0.5
AUDIO_CONFIDENCE_THRESHOLD=0.3

# Transcription Engine Configuration
TRANSCRIPTION_ENGINE_TYPE=sequential
TRANSCRIPTION_BATCH_SIZE=4
TRANSCRIPTION_ENABLE_MIXED_PRECISION=true
TRANSCRIPTION_USE_CACHE=true
TRANSCRIPTION_CHUNK_DURATION_SECONDS=30.0

# CUDA Configuration
CUDA_ENABLED=true
CUDA_MIXED_PRECISION=fp16
# PyTorch CUDA memory allocation configuration (reduces fragmentation)
# Use PYTORCH_ALLOC_CONF (new) instead of PYTORCH_CUDA_ALLOC_CONF (deprecated)
PYTORCH_ALLOC_CONF=expandable_segments:True

# Production Settings (uncomment for production)
# HOST=0.0.0.0
# PORT=8000
# DEBUG=false
# CORS_ORIGINS=https://your-frontend-domain.com
# CUDA_ENABLED=true